{"cells":[{"metadata":{"id":"AE307F25281C458EA537D58895A4862D","mdEditEnable":false},"cell_type":"markdown","source":["### 导入数据"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"40B05D25EF1740FB867EF4E6480D6C6A","collapsed":false,"scrolled":false,"hide_input":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['video_create_log.txt',\n 'user_activity_log.txt',\n 'user_register_log.txt',\n 'app_launch_log.txt']"}}],"source":["import pandas as pd\n","import numpy as np\n","import os \n","# path='/Users/chizhu/data/competition_data/快手活跃用户.tar/'\n","path='/mnt/datasets/fusai/'\n","os.listdir(path)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"8DBCD63462B0463D888D7B9BB78190C0","collapsed":false,"scrolled":false,"hide_input":false},"outputs":[],"source":["action=pd.read_csv(path+\"user_activity_log.txt\",sep=\"\\t\",header=None,dtype={0: np.uint32, 1: np.uint8, 2: np.uint8, 3: np.uint32, 4: np.uint32, 5: np.uint8})\n","action.columns=['user_id','day','page','video_id','author_id',\"action_type\"]\n","\n","register=pd.read_csv(path+\"user_register_log.txt\",sep=\"\\t\",header=None,dtype={0: np.uint32, 1: np.uint8, 2: np.uint8, 3: np.uint16})\n","register.columns=['user_id','register_day','register_type',\"device_type\"]\n","\n","launch=pd.read_csv(path+\"app_launch_log.txt\",sep=\"\\t\",header=None,dtype={0: np.uint32, 1: np.uint8})\n","launch.columns=['user_id','day']\n","\n","video=pd.read_csv(path+\"video_create_log.txt\",sep=\"\\t\",header=None,dtype={0: np.uint32, 1: np.uint8})\n","video.columns=['user_id','day']\n","action=action.sort_values(by=['user_id',\"day\"])\n","launch=launch.sort_values(by=['user_id',\"day\"])\n","video=video.sort_values(by=['user_id',\"day\"])\n","register=register.sort_values(by=['user_id',\"register_day\"])"]},{"metadata":{"id":"9206C340DAC54B1E861CE521EA8B8BA7","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":["try:\n","    from joblib import Parallel, delayed\n","except ImportError:\n","    import os\n","    os.system('pip install joblib')\n","def applyParallel( func,df,pred_firstday):\n","    retLst=Parallel(n_jobs=-1)(delayed(func)(df[df.user_id==userid],pred_firstday) for userid in df.user_id.unique())\n","    return pd.concat(retLst, axis=0)"],"execution_count":34},{"metadata":{"id":"FA4ECC6AF90E435C9B42302BACAEE4D2","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":["def get_author_num(df):\n","    return len(df['user_id'].unique())\n","def get_author_user_count_feat(action,train_begin,register_lastday):\n","    action = action[(action.day>=train_begin)&(action.day<=register_lastday)]\n","    a=action.drop_duplicates(['user_id','author_id'])\n","    b=action.groupby(\"author_id\",as_index=False).agg(get_author_num)[['author_id',\"user_id\"]]\n","    b.columns=['author_id',\"user_count\"]\n","    c=pd.merge(a,b,on=\"author_id\",how=\"left\")\n","    res=c.groupby(\"user_id\",as_index=False)['user_count'].agg({\"user_count_max\":\"max\",\"user_count_min\":\"min\",\n","    \"user_count_mean\":\"mean\",\"user_count_skew\":\"skew\",\"user_count_std\":\"std\"\n","})\n","    \n","    return res"],"execution_count":35},{"metadata":{"id":"3580B6AE4C354A9681C97F1DE2C62216","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":["%%time\n","data3_author=get_author_user_count_feat(action,15,30)"],"execution_count":36},{"metadata":{"id":"2FD71AD9BCC54E2F87DA34C40B8E3DEF","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":["data3_author.to_csv(\"data3_author.csv\",index=False)"],"execution_count":37},{"metadata":{"id":"72ED12225A4940FF9F1E01096F697875","mdEditEnable":false},"cell_type":"markdown","source":["### 特征提取"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"A5189A0888C444C38F27278F8A2A8C56","collapsed":false,"scrolled":false,"hide_input":false},"outputs":[],"source":["\n","import datetime\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def get_action_type_feat(df,i):\n","    tm=pd.DataFrame(index=range(1))\n","    df.index=range(len(df))\n","    a=df[df.action_type==i]\n","    a['index']=a.index\n","    if len(a)<=1:\n","        tm['action_type_'+str(i)+\"_day_dis_max\"]=np.nan\n","        tm['action_type_'+str(i)+\"_day_dis_min\"]=np.nan\n","        tm['action_type_'+str(i)+\"_day_dis_mean\"]=np.nan\n","        tm['action_type_'+str(i)+\"_day_dis_median\"]=np.nan\n","        tm['action_type_'+str(i)+\"_day_dis_std\"]=np.nan\n","        tm['action_type_'+str(i)+\"_day_dis_skew\"]=np.nan\n","        tm['action_type_'+str(i)+\"_day_dis_kurt\"]=np.nan\n","        tm['action_type_'+str(i)+\"_day_dis_last\"]=np.nan\n","        ###\n","        tm['action_type_'+str(i)+\"_dis_max\"]=np.nan\n","        tm['action_type_'+str(i)+\"_dis_min\"]=np.nan\n","        tm['action_type_'+str(i)+\"_dis_mean\"]=np.nan\n","        tm['action_type_'+str(i)+\"_dis_median\"]=np.nan\n","        tm['action_type_'+str(i)+\"_dis_std\"]=np.nan\n","        tm['action_type_'+str(i)+\"_dis_skew\"]=np.nan\n","        tm['action_type_'+str(i)+\"_dis_kurt\"]=np.nan\n","        tm['action_type_'+str(i)+\"_dis_last\"]=np.nan\n","    else:\n","        a_diff=a.diff(1)\n","        tm['action_type_'+str(i)+\"_day_dis_max\"]=a_diff['day'].max()\n","        tm['action_type_'+str(i)+\"_day_dis_min\"]=a_diff['day'].min()\n","        tm['action_type_'+str(i)+\"_day_dis_mean\"]=a_diff['day'].mean()\n","        tm['action_type_'+str(i)+\"_day_dis_median\"]=a_diff['day'].median()\n","        tm['action_type_'+str(i)+\"_day_dis_std\"]=a_diff['day'].std()\n","        tm['action_type_'+str(i)+\"_day_dis_skew\"]=a_diff['day'].skew()\n","        tm['action_type_'+str(i)+\"_day_dis_kurt\"]=a_diff['day'].kurt()\n","        tm['action_type_'+str(i)+\"_day_dis_last\"]=a_diff['day'].values[-1]\n","        #####\n","        tm['action_type_'+str(i)+\"_dis_max\"]=a_diff['index'].max()\n","        tm['action_type_'+str(i)+\"_dis_min\"]=a_diff['index'].min()\n","        tm['action_type_'+str(i)+\"_dis_mean\"]=a_diff['index'].mean()\n","        tm['action_type_'+str(i)+\"_dis_median\"]=a_diff['index'].median()\n","        tm['action_type_'+str(i)+\"_dis_std\"]=a_diff['index'].std()\n","        tm['action_type_'+str(i)+\"_dis_skew\"]=a_diff['index'].skew()\n","        tm['action_type_'+str(i)+\"_dis_kurt\"]=a_diff['index'].kurt()\n","        tm['action_type_'+str(i)+\"_dis_last\"]=a_diff['index'].values[-1]\n","        \n","    return tm\n","        \n","\n","def get_inpred_count(temp,flag):\n","    count=0\n","    for i in [flag+'_lastdaytopred+day_dis_mean',flag+'_lastdaytopred+day_dis_max',flag+\"_lastdaytopred+day_dis_min\",\n","    flag+\"_lastdaytopred+day_dis_median\"]:\n","        if temp[i].values[0]>0 and temp[i].values[0]<=7:\n","            count+=1\n","    return count\n","def get_launch_feat(df,pred_firstday):\n","    temp=pd.DataFrame(index=range(1))\n","    temp['user_id']=df['user_id'].unique()[0]\n","    temp['launch_count']=len(df)\n","    temp['launch_day_mean']=df['dur_day'].mean()\n","    temp['launch_day_median']=df['dur_day'].median()\n","    temp['launch_day_min']=df['dur_day'].min()\n","    temp['launch_day_max']=df['dur_day'].max()\n","    temp['launch_day_max-min']=temp['launch_day_max']-temp['launch_day_min']\n","    last_7days_len=len(df[df['dur_day']<=7])\n","    temp['last_7day_launch_rate']=last_7days_len/temp['launch_count']\n","    last_5days_len=len(df[df['dur_day']<=5])\n","    temp['last_5day_launch_rate']=last_5days_len/temp['launch_count']\n","    last_4days_len=len(df[df['dur_day']<=4])\n","    temp['last_4day_launch_rate']=last_4days_len/temp['launch_count']\n","    last_3days_len=len(df[df['dur_day']<=3])\n","    temp['last_3day_launch_rate']=last_3days_len/temp['launch_count']\n","    last_2days_len=len(df[df['dur_day']<=2])\n","    \n","    temp['last_7day_launch_cnt']=last_7days_len/7\n","    temp['last_5day_launch_cnt']=last_5days_len/5\n","    temp['last_4day_launch_cnt']=last_4days_len/4\n","    temp['last_3day_launch_cnt']=last_3days_len/3\n","    temp['last_2day_launch_cnt']=last_2days_len/2\n","    \n","    temp['launch_day_std']=df['dur_day'].std()\n","    temp['launch_day_skew']=df['dur_day'].skew()\n","    temp['launch_day_kurt']=df['dur_day'].kurt()\n","    # temp['last_launch_day_before_pred']=pred_firstday-df['day'].values[-1]\n","    df_diff=df.diff(1)\n","    temp['launch_day_dis_max']=df_diff['day'].max()\n","    temp['launch_day_dis_min']=df_diff['day'].min()\n","    temp['launch_day_dis_mean']=df_diff['day'].mean()\n","    temp['launch_day_dis_median']=df_diff['day'].median()\n","    temp['launch_day_dis_std']=df_diff['day'].std()\n","    temp['launch_day_dis_skew']=df_diff['day'].skew()\n","    temp['launch_day_dis_kurt']=df_diff['day'].kurt()\n","    temp['launch_day_dis_last']=df_diff['day'].values[-1]\n","     \n","    temp['launch_lastdaytopred+day_dis_mean']=0-temp['launch_day_min'].values[0]+temp['launch_day_dis_mean'].values[0]\n","    \n","    temp['launch_lastdaytopred+day_dis_max']=0-temp['launch_day_min'].values[0]+temp['launch_day_dis_max'].values[0]\n","    temp['launch_lastdaytopred+day_dis_min']=0-temp['launch_day_min'].values[0]+temp['launch_day_dis_min'].values[0]\n","    temp['launch_lastdaytopred+day_dis_median']=0-temp['launch_day_min'].values[0]+temp['launch_day_dis_median'].values[0]\n","    temp[\"launch_inpredday_count\"]=get_inpred_count(temp,\"launch\")\n","    temp['launch_sum']=0\n","    for i in range(pred_firstday-7,pred_firstday):\n","        if i in df['day'].values:\n","            temp['last_'+str(pred_firstday-i)+\"_launch_num\"]=1\n","        else :\n","            temp['last_'+str(pred_firstday-i)+\"_launch_num\"]=0\n","        temp['launch_sum']+=temp['last_'+str(pred_firstday-i)+\"_launch_num\"]/(pred_firstday-i)\n","    for i in [2,3,4,5,6,7]:\n","        temp[\"launch_\"+str(i)+\"_continuous_day_count\"]=get_continuous_day_count(i,df)\n","    \n","    return temp\n","\n","\n","def get_continuous_day_count(cont_num,df):\n","    day_set=df['day'].unique()\n","\n","    day_count=0\n","    for i in day_set:\n","        flag=0\n","        for j in range(1,cont_num):\n","            if i+j in day_set:\n","                flag+=1\n","       \n","        if flag==cont_num-1:\n","            day_count+=1\n","    \n","    \n","    return day_count\n","            \n","    \n","    \n","def get_video_feat(df,pred_firstday):\n","    temp=pd.DataFrame(index=range(1))\n","    temp['user_id']=df['user_id'].unique()[0]\n","    temp['video_count']=len(df)\n","    \n","    temp['day_video_mean']=df.groupby('day',as_index=False).count()['user_id'].mean()\n","    temp['day_video_median']=df.groupby('day',as_index=False).count()['user_id'].median()\n","    temp['day_video_std']=df.groupby('day',as_index=False).count()['user_id'].std()\n","    temp['day_video_max']=df.groupby('day',as_index=False).count()['user_id'].max()\n","    temp['day_video_min']=df.groupby('day',as_index=False).count()['user_id'].min()\n","    temp['day_video_skew']=df.groupby('day',as_index=False).count()['user_id'].skew()\n","    temp['day_video_kurt']=df.groupby('day',as_index=False).count()['user_id'].kurt()\n","    temp['day_video_last']=df.groupby('day',as_index=False).count()['user_id'].values[-1]\n","    b=dict(zip(df.groupby('day',as_index=False).count()['day'].values,df.groupby('day',as_index=False).count()['user_id'].values))\n","    temp['video_sum']=0\n","    for i in range(pred_firstday-7,pred_firstday):\n","        if i in b.keys():\n","            temp['last_'+str(pred_firstday-i)+\"_video_num\"]=b[i]\n","        else :\n","            temp['last_'+str(pred_firstday-i)+\"_video_num\"]=0\n","        temp['video_sum']+=temp['last_'+str(pred_firstday-i)+\"_video_num\"]/(pred_firstday-i)\n","    temp['video_day_count']=df.groupby('day',as_index=False).count()['user_id'].count()\n","    df_temp=df.groupby('day',as_index=False).count()\n","    df_temp['dur_day']=df_temp['day'].apply(lambda x:pred_firstday-x,1)\n","    \n","    temp['video_day_mean']=df_temp['dur_day'].mean()\n","    temp['video_day_median']=df_temp['dur_day'].median()\n","    temp['video_day_min']=df_temp['dur_day'].min()\n","    temp['video_day_max']=df_temp['dur_day'].max()\n","    temp['video_day_max-min']=temp['video_day_max']-temp['video_day_min']\n","    last_7days_len=len(df_temp[df_temp['dur_day']<=7])\n","    temp['last_7day_video_rate']=last_7days_len/len(df_temp)\n","    \n","    last_5days_len=len(df_temp[df_temp['dur_day']<=5])\n","    temp['last_5day_video_rate']=last_5days_len/len(df_temp)\n","    last_4days_len=len(df_temp[df_temp['dur_day']<=4])\n","    temp['last_4day_video_rate']=last_4days_len/len(df_temp)\n","    last_3days_len=len(df_temp[df_temp['dur_day']<=3])\n","    temp['last_3day_video_rate']=last_3days_len/len(df_temp)\n","    last_2days_len=len(df_temp[df_temp['dur_day']<=2])\n","    \n","    temp['last_7day_video_cnt']=last_7days_len/7\n","    temp['last_5day_video_cnt']=last_5days_len/5\n","    temp['last_4day_video_cnt']=last_4days_len/4\n","    temp['last_3day_video_cnt']=last_3days_len/3\n","    temp['last_2day_video_cnt']=last_2days_len/2\n","   \n","    \n","    temp['video_day_std']=df_temp['dur_day'].std()\n","    temp['video_day_skew']=df_temp['dur_day'].skew()\n","    temp['video_day_kurt']=df_temp['dur_day'].kurt()\n","    # temp['last_video_day_before_pred']=pred_firstday-df_temp['day'].values[-1]\n","    df_diff=df_temp.diff(1)\n","    temp['video_day_dis_max']=df_diff['day'].max()\n","    temp['video_day_dis_min']=df_diff['day'].min()\n","    temp['video_day_dis_mean']=df_diff['day'].mean()\n","    temp['video_day_dis_median']=df_diff['day'].median()\n","    temp['video_day_dis_std']=df_diff['day'].std()\n","    temp['video_day_dis_skew']=df_diff['day'].skew()\n","    temp['video_day_dis_kurt']=df_diff['day'].kurt()\n","    temp['video_day_dis_last']=df_diff['day'].values[-1]\n","    \n","    \n","    for i in [2,3,4,5,6,7]:\n","        temp[\"video_\"+str(i)+\"_continuous_day_count\"]=get_continuous_day_count(i,df_temp)\n","    \n","    return temp\n","  \n","    \n","    \n","def get_action_feat(df,pred_firstday):\n","    temp=pd.DataFrame(index=range(1))\n","    temp['user_id']=df['user_id'].values[0]\n","    temp['action_count']=len(df)\n","    \n","    t1=df.groupby('day',as_index=False).count()\n","    temp['day_action_mean']=t1['user_id'].mean()\n","    temp['day_action_median']=t1['user_id'].median()\n","    temp['day_action_std']=t1['user_id'].std()\n","    temp['day_action_max']=t1['user_id'].max()\n","    temp['day_action_min']=t1['user_id'].min()\n","    temp['day_action_skew']=t1['user_id'].skew()\n","    temp['day_action_kurt']=t1['user_id'].kurt()\n","    temp['day_action_last']=t1['user_id'].values[-1]\n","   \n","    ####author\n","   \n","    t2=df.groupby(\"author_id\",as_index=False).count()\n","    temp['author_min']=t2['user_id'].min()\n","    temp['author_max']=t2['user_id'].max()\n","    temp['author_mean']=t2['user_id'].mean()\n","    temp['author_median']=t2['user_id'].median()\n","    temp['author_std']=t2['user_id'].std()\n","    temp['author_skew']=t2['user_id'].skew()\n","    temp['author_kurt']=t2['user_id'].kurt()\n","    temp['author_visit_cnt']=len(df['author_id'].unique())/temp['action_count']\n","    \n","    b=dict(zip(t1['day'].values,t1['user_id'].values))\n","    temp['action_sum']=0\n","    for i in range(pred_firstday-7,pred_firstday):\n","        if i in b.keys():\n","            temp['last_'+str(pred_firstday-i)+\"_action_num\"]=b[i]\n","        else :\n","            temp['last_'+str(pred_firstday-i)+\"_action_num\"]=0\n","        temp['action_sum']+=temp['last_'+str(pred_firstday-i)+\"_action_num\"]/(pred_firstday-i)\n","    temp['action_day_count']=len(t1)\n","    \n","    df_temp=t1\n","    df_temp['dur_day']=df_temp['day'].apply(lambda x:pred_firstday-x,1)\n","\n","    temp['action_day_mean']=df_temp['dur_day'].mean()\n","    temp['action_day_median']=df_temp['dur_day'].median()\n","    temp['action_day_min']=df_temp['dur_day'].min()\n","    temp['action_day_max']=df_temp['dur_day'].max()\n","    temp['action_day_max-min']=temp['action_day_max']-temp['action_day_min']\n","    last_7days_len=len(df_temp[df_temp['dur_day']<=7])\n","    temp['last_7day_action_rate']=last_7days_len/temp['action_day_count']\n","    \n","    last_5days_len=len(df_temp[df_temp['dur_day']<=5])\n","    temp['last_5day_action_rate']=last_5days_len/temp['action_day_count']\n","    last_4days_len=len(df_temp[df_temp['dur_day']<=4])\n","    temp['last_4day_action_rate']=last_4days_len/temp['action_day_count']\n","    last_3days_len=len(df_temp[df_temp['dur_day']<=3])\n","    temp['last_3day_action_rate']=last_3days_len/temp['action_day_count']\n","    last_2days_len=len(df_temp[df_temp['dur_day']<=2])\n","#     temp['last_2day_action_rate']=last_2days_len/len(df)\n","#     last_1days_len=len(df_temp[df_temp['dur_day']<=1])\n","#     temp['last_1day_action_rate']=last_1days_len/len(df)\n","\n","    temp['last_7day_action_cnt']=last_7days_len/7\n","    temp['last_5day_action_cnt']=last_5days_len/5\n","    temp['last_4day_action_cnt']=last_4days_len/4\n","    temp['last_3day_action_cnt']=last_3days_len/3\n","    temp['last_2day_action_cnt']=last_2days_len/2\n","    \n","    temp['action_day_std']=df_temp['dur_day'].std()\n","    temp['action_day_skew']=df_temp['dur_day'].skew()\n","    temp['action_day_kurt']=df_temp['dur_day'].kurt()\n","    # temp['last_action_day_before_pred']=pred_firstday-df_temp['day'].values[-1]\n","  \n","    starttime = datetime.datetime.now()\n","    df_diff=df_temp.diff(1)\n","    temp['action_day_dis_max']=df_diff['day'].max()\n","    temp['action_day_dis_min']=df_diff['day'].min()\n","    temp['action_day_dis_mean']=df_diff['day'].mean()\n","    temp['action_day_dis_median']=df_diff['day'].median()\n","    temp['action_day_dis_std']=df_diff['day'].std()\n","    temp['action_day_dis_skew']=df_diff['day'].skew()\n","    temp['action_day_dis_last']=df_diff['day'].values[-1]\n","    \n","    temp['action_lastdaytopred+day_dis_mean']=0-temp['action_day_min'].values[0]+temp['action_day_dis_mean'].values[0]\n","    \n","    temp['action_lastdaytopred+day_dis_max']=0-temp['action_day_min'].values[0]+temp['action_day_dis_max'].values[0]\n","    temp['action_lastdaytopred+day_dis_min']=0-temp['action_day_min'].values[0]+temp['action_day_dis_min'].values[0]\n","    temp['action_lastdaytopred+day_dis_median']=0-temp['action_day_min'].values[0]+temp['action_day_dis_median'].values[0]\n","    temp[\"action_inpredday_count\"]=get_inpred_count(temp,\"action\")\n","   \n","    for i in [2,3,4,5,6,7]:\n","        temp[\"action_\"+str(i)+\"_continuous_day_count\"]=get_continuous_day_count(i,df_temp)\n","   \n","    ###page\n","    temp['page_type_count']=len(df.page.unique())\n","    for i in range(5):\n","        temp['page_'+str(i)+\"_count\"]=len(df[df.page==i])/temp['action_count']\n","    ###action_type\n","    temp['action_type_count']=len(df.action_type.unique())\n","    for i in range(6):\n","        temp['action_type_'+str(i)+\"_count\"]=len(df[df.action_type==i])/temp['action_count']\n","   \n","    #####action_type_other\n","    for i in range(1,6):\n","        temp_other=get_action_type_feat(df,i)\n","        temp=temp.join(temp_other)\n","        \n","    df['id_same']=df[\"user_id\"]-df['author_id']\n","    a=len(df[df['id_same']==0])\n","    temp['author_and_id_the_same']=a/temp['action_count']\n","    \n","    \n","    return temp   \n","\n","\n","\n","def get_action_author_feat(action,register):\n","    author_temp=action.groupby(['author_id',\"user_id\"],as_index=False).count()\n","    author=author_temp.groupby(\"author_id\",as_index=False).count()[['author_id','user_id']]\n","    author.rename(columns={'user_id':'author_count', 'author_id':'user_id', }, inplace = True)\n","    id_list=register[['user_id']]\n","    action_other_feat=pd.merge(id_list,author,on=\"user_id\",how=\"left\")\n","    \n","    star=action[action.action_type==2]\n","    star=star[['author_id','action_type']]\n","    star.columns=['user_id','followers']\n","    star_count=star.groupby(\"user_id\",as_index=False).count()\n","    action_other_feat=pd.merge(action_other_feat,star_count,on=\"user_id\",how=\"left\")  \n","       \n","    \n","    return action_other_feat\n","\n","\n","def get_label(data,launch,pred_begin=None,pred_end=None):\n","    print(\"get label...\")\n","    starttime = datetime.datetime.now()\n","    launch_data=launch[(launch.day>=pred_begin)&(launch.day<=pred_end)]\n","    target=launch_data.groupby(\"user_id\",as_index=False).count()[['user_id']]\n","    target['label']=1\n","    train=pd.merge(data,target,on=\"user_id\",how=\"left\")\n","    train['label']=train['label'].fillna(0)\n","    train['label']=train['label'].astype(int)\n","    \n","    print(\"use time:\",datetime.datetime.now()-starttime )\n","   \n","    \n","    return train\n","\n","def get_data(action,video,register,launch,register_lastday,train_begin,pred_firstday):\n","    register_data=register[register.register_day<=register_lastday]\n","    launch_data=launch[(launch.day>=train_begin)&(launch.day<=register_lastday)]\n","    video_data=video[(video.day>=train_begin)&(video.day<=register_lastday)]\n","    action_data = action[(action.day>=train_begin)&(action.day<=register_lastday)]\n","    \n","    register_data['register_to_predday']=register_data.apply(lambda x:pred_firstday-x['register_day'],1)\n","    \n","    print(\"process launch...\")\n","    starttime = datetime.datetime.now()\n","    launch_data['dur_day']=launch_data['day'].apply(lambda x:pred_firstday-x,1)\n","#     launch_feat=launch_data.groupby(\"user_id\").apply(lambda x:get_launch_feat(x,pred_firstday))\n","#     launch_feat.index=range(len(launch_feat))\n","    launch_feat=applyParallel(get_launch_feat,launch_data,pred_firstday)\n","    print(\"use time:\",datetime.datetime.now()-starttime )\n","    \n","    print(\"process video...\")\n","    starttime = datetime.datetime.now()\n","#     video_feat=video_data.groupby(\"user_id\").apply(lambda x:get_video_feat(x,pred_firstday))\n","#     video_feat.index=range(len(video_feat))\n","    video_feat=applyParallel(get_video_feat,video_data,pred_firstday)\n","    print(\"use time:\",datetime.datetime.now()-starttime )\n","    \n","    print(\"process action...\")\n","    starttime = datetime.datetime.now()\n","    # action_feat=action_data.groupby(\"user_id\").apply(lambda x:get_action_feat(x,pred_firstday))\n","    # action_feat.index=range(len(action_feat))\n","    action_feat=applyParallel(get_action_feat,action_data,pred_firstday)\n","    print(\"use time:\",datetime.datetime.now()-starttime )\n","    \n","    print(\"process action other...\")\n","    starttime = datetime.datetime.now()\n","    action_other=get_action_author_feat(action_data,register_data)\n","    print(\"use time:\",datetime.datetime.now()-starttime )\n","    \n","    ####merge\n","    \n","    data=pd.merge(register_data,launch_feat,on=\"user_id\",how=\"left\")\n","    data=pd.merge(data,video_feat,on=\"user_id\",how=\"left\")\n","    data=pd.merge(data,action_feat,on=\"user_id\",how=\"left\")\n","    data=pd.merge(data,action_other,on=\"user_id\",how=\"left\")\n","    \n","    \n","    \n","    return data\n","    \n","    "]},{"metadata":{"id":"E59707169E354738988A100D57215CF4","mdEditEnable":false},"cell_type":"markdown","source":["### 数据集的构建"]},{"metadata":{"id":"3B48626DCF9149E38BED8C7CD860EFEA","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":["import numpy as np\n","def get_more_fast(train_data):\n","    for i in train_data.columns:\n","        if type(train_data[i][0])==np.int64:\n","            train_data[i] = train_data[i].astype(np.int32)\n","        elif type(train_data[i][0])==np.float64:\n","            train_data[i] = train_data[i].astype(np.float32)\n","    return train_data"],"execution_count":5},{"cell_type":"code","execution_count":58,"metadata":{"id":"1B5D457AC7924CC185B5775A3E0F5B60","collapsed":false,"scrolled":false,"hide_input":false},"outputs":[],"source":["%%time\n","####data1\n","data1=get_data(action,video,register,launch,16,1,17)\n","data1=get_label(data1,launch,17,23)"]},{"metadata":{"id":"6D2E57F735A5466D8A264A3D57F53F97","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":["data1=get_more_fast(data1)\n","data1.to_csv(\"data1_new2.csv\",index=False)"],"execution_count":61},{"cell_type":"code","execution_count":59,"metadata":{"id":"4F43637B6D04416182556B9CDE1C4BE0","collapsed":false,"scrolled":false,"hide_input":false},"outputs":[],"source":["%%time\n","####data2\n","data2=get_data(action,video,register,launch,23,8,24)\n","data2=get_label(data2,launch,24,30)"]},{"metadata":{"id":"8CB45C14D6014EE0BF4D45D646636237","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":["data2=get_more_fast(data2)\n","data2.to_csv(\"data2_new2.csv\",index=False)"],"execution_count":60},{"cell_type":"code","execution_count":6,"metadata":{"id":"D8A2DE467CA848779FDC0C3A57EF0ED4","collapsed":false,"scrolled":false,"hide_input":false},"outputs":[],"source":["%%time\n","####data3\n","data3=get_data(action,video,register,launch,30,15,31)"]},{"metadata":{"id":"B746EAF4078D41DE856152E98F819B58","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":["data3=get_more_fast(data3)\n","data3.to_csv(\"/home/kesci/input/data3_new2.csv\",index=False)"],"execution_count":7},{"metadata":{"id":"7D13426C03B4439E97A86D39E5970FB0","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":["######压缩与解压\n","\n","# !tar -jcvf  lgb.tar.bz2 /home/kesci/input\n",""],"execution_count":252},{"metadata":{"id":"C9CA25F0D0C3459E96D8A3B94DD1D206","mdEditEnable":false},"cell_type":"markdown","source":["#### NOTE\n","* lgb+all_feat+leaves_36+feat0.5+nround_900 =>0.91013317\n","* lgb+all_feat+leaves_111+feat0.5+nround_900 => 0.91024836\n","* lgb+all_feat+leaves_132+feat0.5+nround_1000 => 0.91027567\n","* lgb+all_feat+leaves_132+feat0.5+nround_1500 => 0.91021246\n","* lgb+all_feat+leaves_132+feat0.5+nround_1000  * 0.57+ xgb+all_feat+depth_8+feat_0.5+nround_1100 *0.43 =>0.91035289\n","* lgb+all_feat+leaves_132+feat0.5+nround_1000 only use data2 for train =>0.90841265\n","* xgb+all_feat+depth_8+feat_0.5+nround_1000  =>0.91034\n","* lgb+new_feat.drop(\"register_day\") use top126+leaves_32+feat_0.4+nround_1300 =>0.910638 lgb.txt\n","* lgb+new_feat.drop(\"register_day\"+last_action/video/launch_to_pred) use top120+leaves_32+feat_0.4+nround_1200 =>0.9105 lgb_120.txt \n","* lgb+new_feat.drop(\"register_day\"+last_action/video/launch_to_pred) use top 128+leaves_127+feat_0.4+nround_850=>0.91078  lgb_128.txt\n","* xgb+new_feat.drop(\"register_day\"+last_action/video/launch_to_pred) use top 128+depth_8+feat_0.7+nround_1000 =>0.91086 xgb_128.txt\n","* xgb+new_feat.drop(\"register_day\"+last_action/video/launch_to_pred) use top 127+depth_8+feat_0.7+nround_940 =>0.91083 xgb_127.txt\n","#### B\n","* xgb +106feat+ depth_8+feat_0.7+nround_1200 =>0.91195 xgb_06.txt\n","* lgb 128feat+leaves_127+feat_0.7+bagging_0.7+bagging_freq_3+nround_1200+ xgb +106feat+ depth_8+feat_0.7+nround_1200 mean=>0.91197 bagging.txt\n","* lgb 98feat+leaves_127+feat_0.7+bagging_0.7+bagging_freq_3+nround_1400=>0.91191 lgb_98.txt"]},{"metadata":{"id":"2D9DA6EE1D254B6C8C44E1862C697356","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["%%bash\n","tar -jxvf  data.tar.bz2 -C /home/kesci/"],"execution_count":3},{"metadata":{"id":"C89170746DDC45ED8CE3A754A32C70DD","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["ls /home/kesci/home/kesci/input"],"execution_count":4},{"metadata":{"id":"67352E5F9F814550AD5C50C04A9C873F","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["#####load data\n","import pandas as pd\n","print(\"loading data...\")\n","data1=pd.read_csv(\"/home/kesci/home/kesci/input/data1_new.csv\")\n","data2=pd.read_csv(\"/home/kesci/home/kesci/input/data2_new.csv\")\n","data3=pd.read_csv(\"/home/kesci/home/kesci/input/data3_new.csv\")"],"execution_count":5},{"metadata":{"id":"14A217765573470B81E1270917B04D23","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["data1=pd.merge(data1,data1_author[['user_id',\"launch_day_dis_last\",]],on=\"user_id\",how=\"left\")\n","data2=pd.merge(data2,data2_author[['user_id',\"launch_day_dis_last\",]],on=\"user_id\",how=\"left\")\n","data3=pd.merge(data3,data3_author[['user_id',\"launch_day_dis_last\",]],on=\"user_id\",how=\"left\")\n",""],"execution_count":136},{"metadata":{"id":"7C266E79F8994BB196C54D962BEFD16D","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["data1_author=pd.read_csv(\"/home/kesci/home/kesci/input/data1_new2.csv\")\n","data2_author=pd.read_csv(\"/home/kesci/home/kesci/input/data2_new2.csv\")\n","data3_author=pd.read_csv(\"/home/kesci/home/kesci/input/data3_new2.csv\")"],"execution_count":135},{"cell_type":"code","execution_count":6,"metadata":{"id":"EC716AD710F44971976033FC99E12A4C","collapsed":false,"scrolled":false},"outputs":[],"source":["data12=pd.concat([data1,data2])"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"73FD3728FB9042F1A2C23889B89ECE5B","collapsed":false,"scrolled":false},"outputs":[],"source":["train=data12\n","test=data3"]},{"metadata":{"id":"64BB2F595EBF4A068A0A0482146E00D5","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["import pandas as pd\n","result=pd.read_csv(\"0.6+0.4.txt\",header=None)\n","result.columns=['user_id',\"pred\"]"],"execution_count":27},{"metadata":{"id":"E231AF158B514FAFAFC0D67CD28E6239","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["r=register[register.register_day==30]\n","a30=action[action.day==30][['user_id',\"day\"]]\n","a30.columns=['user_id',\"day30\"]\n","a30=a30.drop_duplicates(\"user_id\")\n","c=pd.merge(r,a30,on=\"user_id\",how=\"left\")\n","d=c[c.day30.notnull()]\n","e=pd.merge(d,result,on=\"user_id\",how=\"left\")"],"execution_count":28},{"metadata":{"id":"4E81D3D6360541A09AE8A40967A3A950","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["e=e.sort_values(by=[\"pred\"],ascending=False)"],"execution_count":29},{"metadata":{"id":"19D6FB7D1F1B465C891E5F30A0738B7E","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["f=e.tail(6000)"],"execution_count":30},{"metadata":{"id":"D639AA427E5D497A832AC3DC7566F0EB","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["need=f.head(3000)[['user_id',\"pred\"]]\n","id=list(need.user_id.values)\n","result.set_index(\"user_id\",inplace=True)\n","result.drop(id,0,inplace=True)\n","result=result.reset_index()\n",""],"execution_count":31},{"metadata":{"id":"0775ED03A1F94FF38401A330F0D5A0D8","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["add=need"],"execution_count":32},{"metadata":{"id":"8E6572AFB7CC453A85D112468FD02441","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["h=add.head(1500)"],"execution_count":36},{"metadata":{"id":"C504084850A24681BB32305ADC7A495A","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["t=add.tail(1500)"],"execution_count":38},{"metadata":{"id":"04A2B7E866C547438C79927E397AF52B","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.5/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \"\"\"Entry point for launching an IPython kernel.\n","name":"stderr"}],"source":["t['pred']=0.0"],"execution_count":40},{"metadata":{"id":"39512DCA489748F99D5ABC1E1A42E5AA","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["submit=pd.concat([result,add])"],"execution_count":43},{"metadata":{"id":"C4802BD3A90F4EFCB3B3BE37C26E4BF8","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["submit.to_csv('final.txt',index=False,header=False)"],"execution_count":44},{"metadata":{"id":"45B60F41F2DF4A07AFCFD0105BDF838B","mdEditEnable":false},"cell_type":"markdown","source":["### 分离特征和标签"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"9B1B7F65D9364932AEEDEEB808DC39D8","collapsed":false,"scrolled":false,"hide_input":false},"outputs":[],"source":["features = [x for x in train.columns if x not in ['user_id', 'label',\"prob\",\n","\"register_day\",\n","\"last_video_day_before_pred\",\"last_launch_day_before_pred\",\"last_action_day_before_pred\",\n","# ###############video####################\n","#  'day_video_mean',\n","#  'day_video_std',\n","#  'day_video_max',\n","#  'day_video_min',\n","#  'day_video_skew',\n","#  'day_video_last',\n","\n","#  'last_7_video_num',\n","#  'last_6_video_num',\n","#  'last_5_video_num',\n","#  'last_4_video_num',\n","#  'last_3_video_num',\n","#  'last_2_video_num',\n","#  'last_1_video_num',\n","# #  'video_day_count',\n","# #  'video_day_mean',\n","# #  'video_day_min',\n","# #  'video_day_max',\n","# #  'video_day_max-min',\n","#  'last_7day_video_rate',\n","#  'last_5day_video_rate',\n","#  'last_3day_video_rate',\n","#  'last_2day_video_rate',\n","#  'last_1day_video_rate',\n","# #  'video_day_std',\n","# #  'video_day_skew',\n","# #  'video_day_dis_max',\n","# #  'video_day_dis_min',\n","# #  'video_day_dis_mean',\n","# #  'video_day_dis_std',\n","# #  'video_day_dis_skew',\n","#  'video_2_continuous_day_count',\n","#  'video_3_continuous_day_count',\n","#  'video_5_continuous_day_count',\n","#  'video_7_continuous_day_count',\n","# ########################################\n","\n","'action_type_1_day_dis_max', 'action_type_1_day_dis_min',\n","       'action_type_1_day_dis_mean', 'action_type_1_day_dis_median',\n","       'action_type_1_day_dis_std', 'action_type_1_day_dis_skew',\n","       'action_type_1_day_dis_kurt', 'action_type_1_day_dis_last',\n","       'action_type_1_dis_max', 'action_type_1_dis_min',\n","       'action_type_1_dis_mean', 'action_type_1_dis_median',\n","       'action_type_1_dis_std', 'action_type_1_dis_skew',\n","       'action_type_1_dis_kurt', 'action_type_1_dis_last',\n","       'action_type_2_day_dis_max', 'action_type_2_day_dis_min',\n","       'action_type_2_day_dis_mean', 'action_type_2_day_dis_median',\n","       'action_type_2_day_dis_std', 'action_type_2_day_dis_skew',\n","       'action_type_2_day_dis_kurt', 'action_type_2_day_dis_last',\n","       'action_type_2_dis_max', 'action_type_2_dis_min',\n","       'action_type_2_dis_mean', 'action_type_2_dis_median',\n","       'action_type_2_dis_std', 'action_type_2_dis_skew',\n","       'action_type_2_dis_kurt', 'action_type_2_dis_last',\n","       'action_type_3_day_dis_max', 'action_type_3_day_dis_min',\n","       'action_type_3_day_dis_mean', 'action_type_3_day_dis_median',\n","       'action_type_3_day_dis_std', 'action_type_3_day_dis_skew',\n","       'action_type_3_day_dis_kurt', 'action_type_3_day_dis_last',\n","       'action_type_3_dis_max', 'action_type_3_dis_min',\n","       'action_type_3_dis_mean', 'action_type_3_dis_median',\n","       'action_type_3_dis_std', 'action_type_3_dis_skew',\n","       'action_type_3_dis_kurt', 'action_type_3_dis_last',\n","       'action_type_4_day_dis_max', 'action_type_4_day_dis_min',\n","       'action_type_4_day_dis_mean', 'action_type_4_day_dis_median',\n","       'action_type_4_day_dis_std', 'action_type_4_day_dis_skew',\n","       'action_type_4_day_dis_kurt', 'action_type_4_day_dis_last',\n","       'action_type_4_dis_max', 'action_type_4_dis_min',\n","       'action_type_4_dis_mean', 'action_type_4_dis_median',\n","       'action_type_4_dis_std', 'action_type_4_dis_skew',\n","       'action_type_4_dis_kurt', 'action_type_4_dis_last',\n","       'action_type_5_day_dis_max', 'action_type_5_day_dis_min',\n","       'action_type_5_day_dis_mean', 'action_type_5_day_dis_median',\n","       'action_type_5_day_dis_std', 'action_type_5_day_dis_skew',\n","       'action_type_5_day_dis_kurt', 'action_type_5_day_dis_last',\n","       'action_type_5_dis_max', 'action_type_5_dis_min',\n","       'action_type_5_dis_mean', 'action_type_5_dis_median',\n","       'action_type_5_dis_std', 'action_type_5_dis_skew',\n","       'action_type_5_dis_kurt', 'action_type_5_dis_last',\n","]]\n","label = 'label'"]},{"metadata":{"id":"EE14EE7E1A564723AF35C855EE674CFF","mdEditEnable":false},"cell_type":"markdown","source":["###  LightGBM"]},{"cell_type":"code","execution_count":202,"metadata":{"id":"F5D023FD2D314658982394122387FC34","collapsed":false,"scrolled":false},"outputs":[],"source":["import lightgbm as lgb\n","import xgboost as xgb\n","from sklearn.metrics import auc, log_loss, roc_auc_score,f1_score,recall_score,precision_score\n","from sklearn.cross_validation import StratifiedKFold\n","\n","kf = StratifiedKFold(train[label], n_folds=5, shuffle=True, random_state=1024)\n","\n","params = {\n","            'boosting_type': 'gbdt',\n","            'metric': {'auc',}, \n","#             'is_unbalance':'True',\n","            'learning_rate' : 0.01, \n","             'verbose': 0,\n","            'num_leaves':32 ,\n","            # 'max_depth':8, \n","            # 'max_bin':10, \n","            # 'lambda_l2': 1, \n","            # 'min_child_weight':50,\n","            'objective': 'binary', \n","            'feature_fraction': 0.4,\n","            'bagging_fraction':0.7, # 0.9是目前最优的\n","            'bagging_freq':3,  # 3是目前最优的\n","#             'min_data': 500,\n","            'seed': 1024,\n","            'nthread': 8,\n","            # 'silent': True,\n","}\n","num_round = 3500\n","early_stopping_rounds = 100"]},{"metadata":{"id":"AF7147781D5A4B998C7CD5C0FB95EFFE","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["train.index=range(len(train))"],"execution_count":219},{"metadata":{"id":"FFF42C7C58C84F6A8E0BDF32092FC1AB","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":["dtrain=lgb.Dataset(train[features], label=train[label])\n","cv_result = lgb.cv(params, dtrain, num_boost_round=4000, stratified=True,\n","                           nfold=5, early_stopping_rounds=100)\n","len(cv_result['auc-mean'])"],"execution_count":119},{"metadata":{"id":"3E0BBBC145CB4982B64A5108012EFC31","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":["best_num_boost_rounds = len(cv_result['auc-mean'])\r\n","mean_test_auc = np.mean(cv_result['auc-mean'][best_num_boost_rounds-6 : best_num_boost_rounds-1])\r\n","mean_test_auc"],"execution_count":120},{"metadata":{"id":"4A4E01F6492C44618FC2DC39FB74742F","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":["aus = []\n","rlt_pred = 0\n","for i,(train_index,test_index) in enumerate(kf):\n","  \n","    tr_x = train[features].reindex(index=train_index, copy=False).values\n","    tr_y = train[label][train_index]\n","    te_x = train[features].reindex(index=test_index, copy=False).values\n","    te_y = train[label][test_index]\n","\n","    d_tr = lgb.Dataset(tr_x, label=tr_y)\n","    d_te = lgb.Dataset(te_x, label=te_y)\n","    model = lgb.train(params, d_tr, num_boost_round=num_round, \n","                      valid_sets=d_te,verbose_eval=200,\n","                              early_stopping_rounds=early_stopping_rounds)\n","    pred = model.predict(te_x, num_iteration=model.best_iteration)\n","    \n","    te_y=te_y.apply(lambda x:1 if x>0 else 0)\n","    a = roc_auc_score(te_y, pred)\n","\n","    t_pred = model.predict(test[features].reindex(copy=False), num_iteration=model.best_iteration)\n","    rlt_pred += t_pred\n","\n","    print (\"idx: \", i) \n","    print (\" auc: %.5f\" % a)\n","#     print \" gini: %.5f\" % g\n","    print (\"best tree num: \", model.best_iteration)\n","    aus.append(a)\n","rlt_pred /=5\n","# test['label']=test['label'].apply(lambda x:1 if x>0 else 0)\n","# print(roc_auc_score(test['label'], rlt_pred))\n","print (\"mean\")\n","print (\"auc:       %s\" % (sum(aus) / 5.0))\n",""],"execution_count":117},{"metadata":{"id":"D83F8AF77178474D88D773B92D8A0602","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["dtrain = lgb.Dataset(train[features], label=train[label])\n","# dtest=lgb.Dataset(train[features], label=train[label])\n","dtest = lgb.Dataset(test[features], label=test[label])\n","model = lgb.train(params, dtrain, num_boost_round=1400, \n","                      valid_sets=[dtrain,dtest],\n","                  verbose_eval=100,\n","                early_stopping_rounds=early_stopping_rounds\n","             )\n","pred = model.predict(test[features], num_iteration=model.best_iteration)\n","\n","# roc_auc_score(test[label], pred)"],"execution_count":204},{"metadata":{"id":"8A23AD000B98430B8552F7588FD781E8","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["#####lgb参数扰动\n","# \"\"\"\"\n","# lgb随机扰动\n","# \"\"\"\n","import lightgbm as lgb\n","import sys,random\n","import pickle\n","import os\n","from sklearn.cross_validation import StratifiedKFold\n","from sklearn.metrics import auc, log_loss, roc_auc_score\n","import datetime\n","\n","# os.mkdir('model')\n","# os.mkdir('preds')\n","\n","def pipeline(iteration,feat_num,learn_rate,min_child,leaves,max_bin,lambda_l1,lambda_l2,feature_fraction,bagging_fraction,bagging_freq,random_seed):\n","      \n","    params = {\n","            'boosting_type': 'gbdt',\n","            'metric': {'auc',}, \n","#           \n","            'learning_rate' : learn_rate, \n","             'verbose': 0,\n","            'num_leaves':leaves ,\n","         \n","            'max_bin':max_bin, \n","            'lambda_l1': lambda_l1, \n","            'lambda_l2': lambda_l2, \n","            'min_child_weight':min_child,\n","            'objective': 'binary', \n","            'feature_fraction': feature_fraction,\n","            'bagging_fraction':bagging_fraction, # 0.9是目前最优的\n","            'bagging_freq':bagging_freq,  # 3是目前最优的\n","            'seed': random_seed,\n","            'nthread': 8,\n","         \n","}\n","    \n","    cv_num_boost_round=4000\n","    early_stopping_rounds=100\n","    cv_nfold=5\n","    stratified=True\n","    features=ff.ix[:feat_num,\"feature\"].values\n","    dtrain=lgb.Dataset(train[features], label=train[label])\n","    cv_result = lgb.cv(params, dtrain, num_boost_round=cv_num_boost_round, stratified=stratified,\n","                           nfold=cv_nfold, early_stopping_rounds=early_stopping_rounds)\n","    nround=len(cv_result['auc-mean'])\n","    print(\"best_nround:\",nround)\n","    model = lgb.train(params, dtrain, num_boost_round=nround, \n","                      valid_sets=[dtrain],\n","                  verbose_eval=200,\n","               \n","             )\n","    pred = model.predict(test[features])\n","    #predict test set\n","    test_result = pd.DataFrame(test['user_id'].values,columns=[\"user_id\"])\n","    test_result[\"probility\"] = pred\n","    test_result.to_csv(\"/home/kesci/input/lgb{0}.csv\".format(iteration),index=None,encoding='utf-8')\n","    \n","        \n","if __name__ == \"__main__\":\n","    # max_bin=[230,235,240,245,250,255,260,265,270,275]\n","    # lambda_l1=[0.01,0.05,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5]\n","    # lambda_l2=list(range(0,20,1))\n","    # min_child=list(range(0,50,2))\n","    # leaves=[15,31,32,33,34,35,36,48,63,64,77,88,99,106,111,115,120,121,127,132]\n","    \n","    # learn_rate=[0.005,0.006,0.007,0.008,0.009,0.01,0.011,0.012,0.013,0.014,\n","    # 0.015,0.016,0.017,0.018,0.019,0.02]\n","    # feature_fraction=[i/1000 for i in range(400,710,5)]\n","    # bagging_fraction=[i/1000 for i in range(600,810,5)]\n","    # bagging_freq=[1,2,3,4,5]\n","    # feat_num=list(range(75,128,1))\n","    # random_seed =list( range(4,2066,50))\n","    \n","    # random.shuffle(feat_num)\n","    # random.shuffle(max_bin)\n","    # random.shuffle(lambda_l1)\n","    # random.shuffle(lambda_l2)\n","    # random.shuffle(min_child)\n","    # random.shuffle(leaves)\n","    # random.shuffle(learn_rate)\n","    # random.shuffle(feature_fraction)\n","    # random.shuffle(bagging_fraction)\n","    # random.shuffle(bagging_freq) \n","    # random.shuffle(random_seed)\n","    \n","    # with open('params.pkl','wb') as f:\n","    #     pickle.dump((feat_num,max_bin,lambda_l1,lambda_l2,min_child,leaves,learn_rate,feature_fraction,bagging_fraction,bagging_freq,random_seed),f)\n","    \n","    with open('params.pkl','rb') as f:\n","        feat_num,max_bin,lambda_l1,lambda_l2,min_child,leaves,learn_rate,feature_fraction,bagging_fraction,bagging_freq,random_seed=pickle.load(f)\n","    \n","    for i in range(25,30,1):\n","        \n","        print(\"iter:\",i)\n","        start=datetime.datetime.now()\n","        print(\"feat_num\",feat_num[i%50],\"rate:\",learn_rate[i%16],\"min_child:\",min_child[i%25],\"leaves:\",leaves[i%20],\"maxbin:\",max_bin[i%10],\"l1:\",lambda_l1[i%13],\"l2:\",lambda_l2[i%20],\"feat:\",feature_fraction[i%60],\"bagging:\",bagging_fraction[i%40],\"freq:\",bagging_freq[i%5],\"seed:\",random_seed[i%40])\n","        pipeline(i,feat_num[i%50],learn_rate[i%16],min_child[i%25],leaves[i%20],max_bin[i%10],lambda_l1[i%13],lambda_l2[i%20],feature_fraction[i%60],bagging_fraction[i%40],bagging_freq[i%5],random_seed[i%40])\n","        print(\"use time:\",datetime.datetime.now()-start)\n",""],"execution_count":225},{"metadata":{"id":"5AD242E71DCF4AAB821634331DF0B5D8","collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":["#### save params:\n",""]},{"metadata":{"id":"14117645E29A4BD79671BC5F16F1CE8D","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["# \"\"\"\n","# 将扰动成绩汇总取平均\n","# \"\"\"\n","import pandas as pd \n","import os\n","pred = pd.read_csv('/home/kesci/input/lgb0.csv')\n","user_id = pred.user_id\n","probility = pred.probility\n","for f in range(1,28):\n","    pred = pd.read_csv('/home/kesci/input/lgb%s.csv'%str(f))\n","    probility += pred.probility\n","\n","probility /= 28\n","\n","# result=pd.DataFrame(columns=['userid','probability'])\n","# result.userid=test_data['id']\n","# result.probability=probility.values\n","# result.to_csv('e:/submit_lida_lgm_bagging0213.csv',sep=',',index=False)   "],"execution_count":229},{"metadata":{"id":"5B1AE6C14BF54382872950978B4617D1","mdEditEnable":false},"cell_type":"markdown","source":["### feature importance"]},{"metadata":{"id":"C778B20A1AD2468084DB9C041335FEE3","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["#####特征重要性\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","f=dict(zip(list(train[features].keys()),model.feature_importance()))\n","f=sorted(f.items(),key=lambda d:d[1], reverse = True)\n","f=pd.DataFrame(f,columns=['feature','imp'])\n","plt.bar(range(len(f)),f.imp)\n","plt.xticks(range(len(f)),f.feature,rotation=70,fontsize=20)\n","fig = plt.gcf()\n","fig.set_size_inches(50, 20)"],"execution_count":205},{"metadata":{"id":"2EE1D1B6AAC44333835C53FE65ED0EC9","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["f.ix[:220,:]"],"execution_count":206},{"metadata":{"id":"A58A414C6E96440BB4AB7A5D36B73923","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["features=f.ix[:167,\"feature\"].values"],"execution_count":30},{"metadata":{"id":"E0AE5C675791483B92E5B75D8AB427CD","mdEditEnable":false},"cell_type":"markdown","source":["### para turning "]},{"metadata":{"id":"36BA2D23F86D4067B10012F3A2E6B900","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":["result={\"best_leaves\":0,\"best_score\":0.0}\n","def turning(num_leaves,train=train,test=test,result=result,params=params):\n","    \n","    params['num_leaves']=num_leaves\n","    dtrain = lgb.Dataset(train[features], label=train[label])\n","    # dtest=lgb.Dataset(train[features], label=train[label])\n","    dtest = lgb.Dataset(test[features], label=test[label])\n","    model = lgb.train(params, dtrain, num_boost_round=10000, \n","                          valid_sets=[dtrain,dtest],\n","                      verbose_eval=10000,\n","                    early_stopping_rounds=early_stopping_rounds\n","                     )\n","    pred = model.predict(test[features], num_iteration=model.best_iteration)\n","    # test['prob']=pred\n","    a=roc_auc_score(test[label], pred)\n","    print(\"num_leaves:%d ==== auc:%.6f\"%(num_leaves,a))\n","    if a>result['best_score']:\n","        print(\"###\"*24)\n","        print(\"auc improved change auc:%.6f to %.6f\"%(result['best_score'],a))\n","        print(\"###\"*24)\n","        result['best_leaves']=num_leaves\n","        result['best_score']=a\n","    else:\n","        print(\"%d leaves didn't improve!!! \"%(num_leaves))\n","\n","for i in range(5,257):\n","    turning(i)"],"execution_count":null},{"metadata":{"id":"7BC2667C4D5A42F786DC3DA59DDCDB23","mdEditEnable":false},"cell_type":"markdown","source":["### XGBoost"]},{"metadata":{"id":"22AFF6C840EB48209E6973C50D52AB2F","collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"code","outputs":[],"source":["import xgboost as xgb\r\n","\r\n","params={\r\n","\t'booster':'gbtree',\r\n","\t'objective': 'binary:logistic',\r\n","#      'is_unbalance':'True',\r\n","# \t'scale_pos_weight': 1500.0/13458.0,\r\n","        'eval_metric': \"auc\",\r\n","    \r\n","\t'gamma':0.1,#0.2 is ok\r\n","\t'max_depth':8,\r\n","# \t'lambda':20,\r\n","    # \"alpha\":5,\r\n","        'subsample':0.7,\r\n","        'colsample_bytree':0.7 ,\r\n","        # 'min_child_weight':2.5, \r\n","        'eta': 0.01,\r\n","    # 'learning_rate':0.01,\r\n","\t'seed':1024,\r\n","\t'nthread':12,\r\n","   \r\n","    }\r\n","\r\n","dtrain = xgb.DMatrix(train[features], label=train[label])\r\n","# dval=xgb.DMatrix(train[features], label=train[label])\r\n","dval=xgb.DMatrix(test[features], label=test[label])\r\n","dtest =xgb.DMatrix(test[features])\r\n","\r\n","watchlist  = [(dtrain,'train'),\r\n","    (dval,'val')\r\n","             ]\r\n","model = xgb.train(params, dtrain, num_boost_round=1200, \r\n","                      evals=watchlist,\r\n","                  verbose_eval=200,\r\n","                  early_stopping_rounds=100\r\n","                 )\r\n","pred = model.predict(dtest)\r\n","\r\n","# roc_auc_score(test[label], pred)"],"execution_count":133},{"metadata":{"id":"D71D779876EF4366B74BAFC92C3E62F8","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["aus = []\n","rlt_pred = 0\n","for i,(train_index,test_index) in enumerate(kf):\n","  \n","    tr_x = train[features].reindex(index=train_index, copy=False)\n","    tr_y = train[label][train_index]\n","    te_x = train[features].reindex(index=test_index, copy=False)\n","    te_y = train[label][test_index]\n","\n","    # tr_y=tr_y.apply(lambda x:1 if x>0 else 0)\n","    # te_y=te_y.apply(lambda x:1 if x>0 else 0)\n","    d_tr = xgb.DMatrix(tr_x, label=tr_y)\n","    d_te = xgb.DMatrix(te_x, label=te_y)\n","    watchlist  = [(d_tr,'train'),\n","    (d_te,'val')\n","             ]\n","    model = xgb.train(params, d_tr, num_boost_round=5500, \n","                      evals=watchlist,verbose_eval=200,\n","                              early_stopping_rounds=100)\n","    pred = model.predict(d_te)\n","    \n","    # te_y=te_y.apply(lambda x:1 if x>0 else 0)\n","    a = roc_auc_score(te_y, pred)\n","\n","    t_pred = model.predict(xgb.DMatrix(test[features]))\n","    rlt_pred += t_pred\n","\n","    print (\"idx: \", i) \n","    print (\" auc: %.5f\" % a)\n","#     print \" gini: %.5f\" % g\n","    aus.append(a)\n","rlt_pred /=5\n","# test['label']=test['label'].apply(lambda x:1 if x>0 else 0)\n","# print(roc_auc_score(test['label'], rlt_pred))\n","print (\"mean\")\n","print (\"auc:       %s\" % (sum(aus) / 5.0))"],"execution_count":18},{"metadata":{"id":"7AB3BB433FA74B4F8A909AB40C918009","mdEditEnable":false},"cell_type":"markdown","source":["#### CatBoost"]},{"metadata":{"id":"74FFCA6068964C188E32181941118FC4","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["import catboost as cat\r\n","from catboost import CatBoostClassifier, Pool, cv\r\n","from sklearn.metrics import accuracy_score,roc_auc_score\r\n","\r\n","cat_model=CatBoostClassifier(\r\n","    iterations=1000,\r\n","                         learning_rate=0.1,\r\n","                         depth=8,\r\n","                        #  l2_leaf_reg=5,\r\n","                        #   eval_set=[test[features],test[label]],\r\n","                        # subsample= 0.7,\r\n","                        #  bootstrap_type='Bernoulli',\r\n","                        #  rsm=None,\r\n","                          random_seed=1024,\r\n","                        #  od_wait=100,\r\n","                        #  od_type=\"Iter\",\r\n","                        #  use_best_model=True,\r\n","                         eval_metric=\"AUC\",\r\n","                        logging_level= 'Silent',\r\n","                       \r\n","                        #  verbose=1,\r\n","                         loss_function='Logloss',\r\n","    )\r\n","cat_model.fit(train[features],train[label])\r\n","pred=cat_model.predict_proba(test[features])[:,1]\r\n","roc_auc_score(test[label],pred)"],"execution_count":21},{"metadata":{"id":"7631DD0E62D04D4E8848E02091A430D2","mdEditEnable":false},"cell_type":"markdown","source":["### NN"]},{"metadata":{"id":"8D7601F914DC4730801E08B22CF7ABCC","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["#######fix the result#######\r\n","import numpy as np\r\n","import tensorflow as tf\r\n","import random as rn\r\n","import os\r\n","import keras\r\n","os.environ['PYTHONHASHSEED'] = '0'\r\n","np.random.seed(42)\r\n","rn.seed(12345)\r\n","session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\r\n","from keras import backend as K\r\n","tf.set_random_seed(1234)\r\n","sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\r\n","K.set_session(sess)\r\n","######################\r\n","from sklearn.metrics import roc_auc_score\r\n","import tensorflow as tf\r\n","from keras import backend as K\r\n","\r\n","def auc(y_true, y_pred):\r\n","    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\r\n","    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\r\n","    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\r\n","    binSizes = -(pfas[1:]-pfas[:-1])\r\n","    s = ptas*binSizes\r\n","    return K.sum(s, axis=0)\r\n","#--------------------------------\r\n","    \r\n","def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\r\n","    y_pred = K.cast(y_pred >= threshold, 'float32')\r\n","    # N = total number of negative labels\r\n","    N = K.sum(1 - y_true)\r\n","    # FP = total number of false alerts, alerts from the negative class labels\r\n","    FP = K.sum(y_pred - y_pred * y_true)\r\n","    return FP/N\r\n","    \r\n","def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\r\n","    y_pred = K.cast(y_pred >= threshold, 'float32')\r\n","    # P = total number of positive labels\r\n","    P = K.sum(y_true)\r\n","    # TP = total number of correct alerts, alerts from the positive class labels\r\n","    TP = K.sum(y_pred * y_true)\r\n","    return TP/P\r\n","    \r\n","class RocAucMetricCallback(keras.callbacks.Callback):\r\n","    def __init__(self, predict_batch_size=1024, include_on_batch=False):\r\n","        super(RocAucMetricCallback, self).__init__()\r\n","        self.predict_batch_size=predict_batch_size\r\n","        self.include_on_batch=include_on_batch\r\n"," \r\n","    def on_batch_begin(self, batch, logs={}):\r\n","        pass\r\n"," \r\n","    def on_batch_end(self, batch, logs={}):\r\n","        if(self.include_on_batch):\r\n","            logs['roc_auc_val']=float('-inf')\r\n","            if(self.validation_data):\r\n","                logs['roc_auc_val']=roc_auc_score(self.validation_data[2], \r\n","                                                  self.model.predict([self.validation_data[0],self.validation_data[1]],\r\n","                                                                     batch_size=self.predict_batch_size))\r\n"," \r\n","    def on_train_begin(self, logs={}):\r\n","        if not ('roc_auc_val' in self.params['metrics']):\r\n","            self.params['metrics'].append('roc_auc_val')\r\n"," \r\n","    def on_train_end(self, logs={}):\r\n","        pass\r\n"," \r\n","    def on_epoch_begin(self, epoch, logs={}):\r\n","        pass\r\n"," \r\n","    def on_epoch_end(self, epoch, logs={}):\r\n","        logs['roc_auc_val']=float('-inf')\r\n","        if(self.validation_data):\r\n","            logs['roc_auc_val']=roc_auc_score(self.validation_data[2], \r\n","                                              self.model.predict([self.validation_data[0],self.validation_data[1]],\r\n","                                                                 batch_size=self.predict_batch_size))\r\n"," \r\n"," \r\n","import numpy as np\r\n","from keras.models import Sequential\r\n","from keras.layers import GRU,LSTM,Input,Dense, Dropout,BatchNormalization,Concatenate,Embedding\r\n","import keras\r\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\r\n","from sklearn.metrics import roc_auc_score\r\n","from keras import metrics\r\n","\r\n","# X_train=train[features].fillna(0)\r\n","# X_test=test[features].fillna(0)\r\n","# X_train = np.reshape(X_train.values, (len(train), 16, 1))\r\n","# X_test = np.reshape(X_test.values, (len(test), 16, 1))\r\n","# X_train=np.array(tr.iloc[:,2:18])\r\n","# X_test=np.array(te.iloc[:,2:18])\r\n","X_train=tr.iloc[:,2:18].values\r\n","X_test=te.iloc[:,2:18].values\r\n","feat_train=train[features].fillna(-1).values\r\n","feat_test=test[features].fillna(-1).values\r\n","y_train=train[label].values\r\n","y_test=test[label].values\r\n","model_checkpoint = ModelCheckpoint(\"/home/kesci/input/nn.h5\", monitor='roc_auc_val', mode='max',\r\n","                                       save_best_only=True, verbose=1, save_weights_only=True)\r\n","\r\n","cb = [\r\n","    RocAucMetricCallback(), # include it before EarlyStopping!\r\n","     model_checkpoint,\r\n","    EarlyStopping(monitor='roc_auc_val',patience=10, verbose=1,mode='max')\r\n","]\r\n","input_tensor=Input(shape=(16,))\r\n","emb=Embedding(2, 64 , input_shape=(16,),\r\n","                         trainable=True)(input_tensor)\r\n","lstm_layer =LSTM(32,activation='tanh',recurrent_dropout=0.2)\r\n","lstm=lstm_layer(emb)\r\n","\r\n","lstm = Dropout(0.2)(lstm)\r\n","feat_input = Input(shape=(128,))\r\n","feat_dense = Dense(128, activation='relu')(feat_input)\r\n","merged = Concatenate()([lstm,feat_dense])\r\n","merged = Dropout(0.2)(merged)\r\n","merged = BatchNormalization()(merged)\r\n","x = Dense(64, activation='relu')(merged)\r\n","x = Dropout(0.2)(x)\r\n","x = BatchNormalization()(x)\r\n","x = Dense(64, activation='relu')(x)\r\n","x = Dropout(0.2)(x)\r\n","x = BatchNormalization()(x)\r\n","x = Dense(32, activation='relu')(x)\r\n","x = Dropout(0.2)(x)\r\n","x = BatchNormalization()(x)\r\n","ouput_layer = Dense(1, activation='sigmoid')(x)\r\n","\r\n","model = keras.models.Model(inputs=[input_tensor,feat_input], outputs=ouput_layer)\r\n","\r\n","model.compile(loss='binary_crossentropy',\r\n","              optimizer='adam',\r\n","              metrics=[auc]) #这里就可以写其他评估标准\r\n","model.summary()\r\n","\r\n","train[label]=train[label].astype(int)\r\n","test[label]=test[label].astype(int)\r\n","model.fit(x=[X_train,feat_train],\r\n","        y=y_train,\r\n","         validation_data=([X_test,feat_test],y_test),\r\n","        batch_size=1024, epochs=1500, verbose=1,\r\n","              callbacks=cb,\r\n","         \r\n","          )"],"execution_count":83},{"metadata":{"id":"0AAB9510EE6449ED8C34810CC18E782F","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["model.load_weights(\"/home/kesci/input/nn.h5\")\n","# pred = model.predict(np.reshape(test[features].fillna(-1).values,(len(test),71,1)), batch_size=2048, verbose=1)\n","pred = model.predict([X_test,test[features].fillna(-1).values], verbose=1)\n","roc_auc_score(test[label],pred)"],"execution_count":84},{"metadata":{"id":"4CFB808CF3A1411982EE9319A4886FCA","mdEditEnable":false},"cell_type":"markdown","source":["#### TIME SEQUENCE FEATURE"]},{"metadata":{"id":"8C91B40A49244EF79D5EC0FB71E11C8A","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["from collections import Counter\n","count=0\n","def get_time_feat(df,pred_firstday):\n","    global count\n","    count+=1\n","    df['day']=df.day.astype(\"str\")\n","    df['day']=df.day.apply(lambda x:pred_firstday+\"_\"+str(x),1)\n","    temp=pd.DataFrame(columns=[pred_firstday+\"_\"+str(i) for i in range(1,31)],index=range(1))\n","    temp[\"user_id\"]=df['user_id'].values[-1]\n","    \n","    l=list(df['day'].values)\n","    day_list=Counter(l)\n","    if len(day_list.keys())==1:\n","        temp.loc[0,list(day_list.keys())]=list(day_list.values())[0]\n","    else:\n","        temp.loc[0,list(day_list.keys())]=list(day_list.values())\n","    if count%10000==0:\n","        print(count)\n","    return temp"],"execution_count":11},{"metadata":{"id":"9F6874ED6A904648BE1D23CE25C27E4C","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["launch_feat=applyParallel(get_time_feat,launch,\"action\")"],"execution_count":12},{"metadata":{"id":"85C69E9699F846769535A846A9F1A9A0","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["temp=launch_feat.iloc[:,0:16]\n","temp['user_id']=launch_feat['user_id']\n","tr=train[['user_id',\"label\"]]\n","tr=pd.merge(tr,temp)"],"execution_count":28},{"metadata":{"id":"1AC3F336FF12491F8F575D72367FF521","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["temp=launch_feat.iloc[:,7:23]\n","temp['user_id']=launch_feat['user_id']\n","te=test[['user_id','label']]\n","te=pd.merge(te,temp)"],"execution_count":31},{"metadata":{"id":"81FD5C21143D4ED0B595DF864EB6983F"},"cell_type":"code","outputs":[],"source":[""],"execution_count":null},{"metadata":{"id":"0415CB474EAB4388AC31E28732CF99B5","mdEditEnable":false},"cell_type":"markdown","source":["## stacking"]},{"metadata":{"id":"6F9C1380568943299281A0D32B9DC30F"},"cell_type":"code","outputs":[],"source":["from __future__ import division\n","\n","import numpy as np\n","from sklearn.cross_validation import StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import cross_validation, linear_model\n","\n","# MAX_ITERS = 1\n","# N_TREES = 5\n","# XGBC_TREES = 5\n","# LM_CV_NUM = 5\n","\n","N_TREES = 500\n","XGBC_TREES = 1500\n","LM_CV_NUM = 100\n","MAX_ITERS = 400\n","\n","INITIAL_PARAMS = {\n","    'LR:one': {'C': 2, 'penalty': 'l2','class_weight':\"balanced\",  'n_jobs': -1,\n","               'max_iter': MAX_ITERS,\n","               },\n","    'XGBC:one': {\n","        'n_estimators': XGBC_TREES,\n","        'scale_pos_weight': 6,\n","        'objective': 'binary:logistic',\n","        'learning_rate': 0.02,\n","        'gamma': 0.7,\n","        'reg_lambda': 800,\n","        'colsample_bytree': 0.7,\n","        'max_depth': 5,\n","        'min_child_weight': 4,\n","        'subsample': 0.7,\n","        },\n","    'XGBC:two': {\n","        'n_estimators': 400,\n","        'scale_pos_weight': 1,\n","        'objective': 'binary:logistic',\n","        'learning_rate': 0.04,\n","        'colsample_bytree': 0.7,\n","        'max_depth': 5,\n","        'subsample': 0.7,\n","        },\n","    'XGBC:three':{\n","        'objective':'binary:logistic',\n","        'n_estimators': 1500,\n","      'min_child_weight':1,\n","        \n","        \"reg_lambda\":11,\n","         \"reg_alpha\":3,\n","            'scale_pos_weight': 1,\n","#             'is_unbalance':'True',\n","            'learning_rate' : 0.01, \n","             \n","        'gamma':0.1,\n","        'max_depth':5,\n","         'subsample': 0.68,\n","        'colsample_bytree': 0.68,\n","        \n","        'seed':1024,\n","         \"random_state\":1024,\n","\t'nthread':7,\n","        \n","    },\n","     \"LightGBM\":{\n","            'boosting_type': 'gbdt',\n","             'n_estimators': 1500,\n","            'metric': 'auc', \n","            'is_unbalance':True,\n","            'learning_rate' : 0.01, \n","             'verbose': 0,\n","          'min_child_weight':1,\n","            \n","            'max_depth':5, \n","            'max_bin':15, \n","          \"reg_lambda\":11,\n","         \"reg_alpha\":3,\n","            'scale_pos_weight': 1,\n","            'objective': 'binary', \n","            'colsample_bytree': 0.68,\n","            'subsample':0.68, # 0.9是目前最优的\n","            'subsample_freq':1,  # 3是目前最优的\n","            'min_data': 500,\n","            'seed': 1024,\n","         \n","            'nthread': 12,\n","#             'silent': True,\n","},\n","    \n","    \n","      \n","       \n","       \n","     \n","       \n","       \n","    'RFC:one': {\n","        # 'n_estimators': N_TREES, 'n_jobs': -1, 'criterion': 'entropy',\n","        # 'min_samples_leaf': 2, 'bootstrap': False,\n","        # 'max_depth': 20, 'min_samples_split': 7, 'max_features': 0.5\n","        # new tune\n","        'n_estimators': 400, 'n_jobs': -1, 'criterion': 'entropy',\n","        'min_samples_leaf': 3, 'bootstrap': False,\n","        'max_depth': 12, 'min_samples_split': 6, 'max_features': 0.14357\n","    },\n","    'ETC:one': {\n","        'n_estimators': N_TREES, 'n_jobs': -1, 'min_samples_leaf': 2,  'criterion': 'entropy',\n","        'max_depth': 20, 'min_samples_split': 5, 'max_features': 0.4,\n","        'bootstrap': False,\n","        },\n","    'GBC:one': {\n","        'n_estimators': int(N_TREES / 2), 'learning_rate': .08, 'max_features': 0.7,\n","        'min_samples_leaf': 51, 'min_samples_split': 125, 'max_depth': 4,\n","    },\n","    }\n","\n","import xgboost as xgb\n","import lightgbm as lgb\n","MODEL_NAME = 'blend_ensemble'\n","\n","\n","def blending(X_org, y_org, test_org,on_val=False,use_single_model=False):\n","    n_folds =5\n","    verbose = True\n","    shuffle = False\n","\n","    X = X_org\n","    y = y_org\n","    X_submission = test_org[features]\n","    \n","    if shuffle:\n","        idx = np.random.permutation(y.size)\n","        X = X[idx]\n","        y = y[idx]\n","\n","    skf = list(StratifiedKFold(y, n_folds))\n","\n","    clfs = [\n","        RandomForestClassifier().set_params(**INITIAL_PARAMS.get(\"RFC:one\", {})),\n","        ExtraTreesClassifier().set_params(**INITIAL_PARAMS.get(\"ETC:one\", {})),\n","        GradientBoostingClassifier().set_params(**INITIAL_PARAMS.get(\"GBC:one\", {})),\n","#         LogisticRegression().set_params(**INITIAL_PARAMS.get(\"LR:one\", {})),\n","        xgb.XGBClassifier().set_params(**INITIAL_PARAMS.get(\"XGBC:two\", {})),\n","        xgb.XGBClassifier().set_params(**INITIAL_PARAMS.get(\"XGBC:one\", {})),\n","        lgb.LGBMClassifier().set_params(**INITIAL_PARAMS.get(\"LightGBM\", {})),\n","         xgb.XGBClassifier().set_params(**INITIAL_PARAMS.get(\"XGBC:three\", {})),\n","\n","        ]\n","\n","\n","    print (\"Creating train and test sets for blending.\")\n","\n","    dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n","    dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))\n","    for j, clf in enumerate(clfs):\n","        print (j, clf)\n","        dataset_blend_test_j = np.zeros((X_submission.shape[0], len(skf)))\n","        \n","        for i, (train, test) in enumerate(skf):\n","            print (\"Fold\", i)\n","            \n","            X_train = X.reindex(index=train, copy=False)\n","            y_train = y[train]\n","            X_test = X.reindex(index=test, copy=False)\n","            y_train=y_train.apply(lambda x:1 if x>0 else 0)\n","            X_train=X_train.fillna(0)\n","            X_test=X_test.fillna(0)\n","            y_test = y[test]\n","            clf.fit(X_train, y_train)\n","            y_submission = clf.predict_proba(X_test)[:,1]\n","            dataset_blend_train[test, j] = y_submission\n","            dataset_blend_test_j[:, i] = clf.predict_proba(X_submission)[:,1]\n","       \n","        dataset_blend_test[:,j] = dataset_blend_test_j.mean(1)\n","        if on_val:\n","            print(\"val auc Score: %.10f\" % (roc_auc_score(test_org.label, dataset_blend_test[:,j]))) \n","\n","    if use_single_model:\n","        return dataset_blend_test\n","    print (\"Blending.\")\n","   # clf = LogisticRegression(C=2, penalty='l2', class_weight='balanced', n_jobs=-1)\n","    clf = linear_model.RidgeCV(\n","        alphas=np.linspace(0, 200), cv=LM_CV_NUM)\n","    # clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=100)\n","    clf.fit(dataset_blend_train, y)\n","    # y_submission = clf.predict_proba(dataset_blend_test)[:,1]\n","#     print clf.coef_, clf.intercept_\n","    y_submission = clf.predict(dataset_blend_test)  # for RidgeCV\n","    \n","    print (\"Linear stretch of predictions to [0,1]\")\n","    y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())\n","    if on_val:\n","        print(\"********4****val auc Score: %0.5f\" % (roc_auc_score(test_org.label, y_submission)))\n","        df=pd.DataFrame(dataset_blend_test,columns=[\"m1\",\"m2\",\"m3\",\"m4\",\"m5\",\"m6\"])\n","        df[\"prob\"]=df['m1']*0.1+df[\"m2\"]*0.1+df['m3']*0.1+df[\"m4\"]*0.2+df['m5']*0.3+df[\"m6\"]*0.2\n","        print(\"val auc Score: %0.5f\" % (roc_auc_score(test_org.label,df['prob'] )))\n","    return y_submission\n","import time\n","start=time.time()\n","y_submission=blending(train[features],train[label],test,on_val=True,use_single_model=True)\n","end=time.time()\n","print(\"time:%.2f s\"%(end-start))\n","print (\"blend result\")\n","# save(test, y_submission, \"xgb\")\n",""],"execution_count":null},{"metadata":{"id":"D388AF9AA91E41AC82E9C30E3E30A556","mdEditEnable":false},"cell_type":"markdown","source":["## bagging"]},{"metadata":{"id":"D7B80A20337F460D8F33A49747930658","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["import pandas as pd\n","result1=pd.read_csv(\"bagging28.txt\",header=None)\n","result1.columns=['user_id',\"pred\"]\n","result2=pd.read_csv(\"bagging.txt\",header=None)\n","result2.columns=['user_id',\"pred\"]\n","# result3=pd.read_csv(\"cat.txt\",header=None)\n","# result3.columns=['user_id',\"pred\"]"],"execution_count":1},{"metadata":{"id":"05AA625E9ED34B6882398DDA3492415A","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["result1[result1['user_id']==547]"],"execution_count":3},{"metadata":{"id":"2B3EA8EF4E59417F966C61750553B3FF","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["result1['prob']=result1[\"pred\"]*0.6+0.4*result2[\"pred\"]"],"execution_count":31},{"metadata":{"id":"13A3CB0A1F0D44198CFF2BB707C0B7B2","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["result1[['user_id',\"prob\"]].to_csv(\"0.6+0.4.txt\",index=False,header=False)"],"execution_count":50},{"metadata":{"id":"0485A342560C40AF8FEC80A65F309BD1","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["test['prob1']=result1['pred']\n","test['prob2']=result2['pred']\n","# test['prob4']=pred\n","# test['prob3']=result3['pred']"],"execution_count":55},{"metadata":{"id":"04C4D8BA901948308A449C3294D98487","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":["#归一化\r\n","min1= test['prob1'].min()\r\n","max1= test['prob1'].max()\r\n","test['prob1'] = test['prob1'].map(lambda x:(x-min1)/(max1-min1))\r\n","\r\n","min2= test['prob2'].min()\r\n","max2= test['prob2'].max()\r\n","test['prob2'] = test['prob2'].map(lambda x:(x-min2)/(max2-min2))\r\n","\r\n","# min3= test['prob3'].min()\r\n","# max3= test['prob3'].max()\r\n","# test['prob3'] = test['prob3'].map(lambda x:(x-min3)/(max3-min3))\r\n","\r\n","# min5= test['prob4'].min()\r\n","# max5= test['prob4'].max()\r\n","# test['prob4'] = test['prob4'].map(lambda x:(x-min5)/(max3-min5))\r\n","\r\n","## 简单加权融合\r\n","submit=pd.DataFrame()\r\n","submit['user_id']=test.user_id\r\n","submit['prob'] = 0.5*test['prob1'] + 0.5*test['prob2']\r\n","\r\n","min4= submit['prob'].min()\r\n","max4= submit['prob'].max()\r\n","submit['prob'] = submit['prob'].map(lambda x:(x-min4)/(max4-min4))\r\n","submit.to_csv('bagging.txt',index=False,header=False)"],"execution_count":57},{"metadata":{"id":"D32638C5736E4CEF87B139556901355C","mdEditEnable":false},"cell_type":"markdown","source":["### output"]},{"cell_type":"code","metadata":{"id":"DD80DF33C6F549608F121E74538D3FD3","collapsed":false,"scrolled":false},"outputs":[],"source":["\n","def save(test, pred, name):\n","    test['prob']=pred\n","    test[['user_id','prob']].to_csv(\"%s.txt\" % ( name), index=False,header=False)\n","    \n","    "],"execution_count":230},{"cell_type":"code","metadata":{"id":"ED88EF6D3F9C4D388B39C8E0F15EA197","collapsed":false,"scrolled":false},"outputs":[],"source":["save(test,probility,\"bagging28\")"],"execution_count":232},{"metadata":{"id":"E18A61B86FB343D386602D3E13937315","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Kesci Submit Tool\nResult File: final.txt (14.84 MiB)\nUploaded.       \n====================\nSubmit Success.\nOK\n\n","name":"stdout"}],"source":["# 下载提交工具至当前目录，仅需执行一次\r\n","# !wget -nv -O kesci_submit https://cdn.kesci.com/submit_tool/v1/kesci_submit&&chmod +x kesci_submit\r\n","# 提交文件my_submission.txt进行评审；温馨提示:本次比赛提交文件的格式为txt\r\n","!./kesci_submit -token 18c6e182512f3cfb -file final.txt"],"execution_count":50}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":2}